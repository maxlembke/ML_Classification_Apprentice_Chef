{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "# A1: Regression Model Development\n",
    "\n",
    "**Author:** Max Lembke \n",
    "\n",
    "**Course:** Machine Learning - DAT-5303\n",
    "\n",
    "**Date:** 02/09/2021\n",
    "\n",
    "<br><hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1) Set-up & Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Import of Libraries & File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>cross_sell_success</th>\n",
       "      <th>name</th>\n",
       "      <th>email</th>\n",
       "      <th>first_name</th>\n",
       "      <th>family_name</th>\n",
       "      <th>total_meals_ordered</th>\n",
       "      <th>unique_meals_purch</th>\n",
       "      <th>contacts_w_customer_service</th>\n",
       "      <th>product_categories_viewed</th>\n",
       "      <th>avg_time_per_site_visit</th>\n",
       "      <th>mobile_number</th>\n",
       "      <th>cancellations_before_noon</th>\n",
       "      <th>cancellations_after_noon</th>\n",
       "      <th>tastes_and_preferences</th>\n",
       "      <th>pc_logins</th>\n",
       "      <th>mobile_logins</th>\n",
       "      <th>weekly_plan</th>\n",
       "      <th>early_deliveries</th>\n",
       "      <th>late_deliveries</th>\n",
       "      <th>package_locker</th>\n",
       "      <th>refrigerated_locker</th>\n",
       "      <th>avg_prep_vid_time</th>\n",
       "      <th>avg_num_meals_customer</th>\n",
       "      <th>master_classes_attended</th>\n",
       "      <th>median_meal_rating</th>\n",
       "      <th>avg_clicks_per_visit</th>\n",
       "      <th>total_photos_viewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>saathos@unitedhealth.com</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>Saathos</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1365.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Alysanne Osgrey</td>\n",
       "      <td>alysanne.osgrey@ge.org</td>\n",
       "      <td>Alysanne</td>\n",
       "      <td>Osgrey</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>40.35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>800.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Edwyd Fossoway</td>\n",
       "      <td>edwyd.fossoway@jnj.com</td>\n",
       "      <td>Edwyd</td>\n",
       "      <td>Fossoway</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>19.77</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>600.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Eleyna Westerling</td>\n",
       "      <td>eleyna.westerling@ge.org</td>\n",
       "      <td>Eleyna</td>\n",
       "      <td>Westerling</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>90.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1490.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Elyn Norridge</td>\n",
       "      <td>elyn.norridge@jnj.com</td>\n",
       "      <td>Elyn</td>\n",
       "      <td>Norridge</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>40.38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>84.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   revenue  cross_sell_success               name                     email first_name family_name  total_meals_ordered  unique_meals_purch  contacts_w_customer_service  product_categories_viewed  avg_time_per_site_visit  mobile_number  cancellations_before_noon  cancellations_after_noon  tastes_and_preferences  pc_logins  mobile_logins  weekly_plan  early_deliveries  late_deliveries  package_locker  refrigerated_locker  avg_prep_vid_time  avg_num_meals_customer  master_classes_attended  median_meal_rating  avg_clicks_per_visit  total_photos_viewed\n",
       "0    393.0                   1            Saathos  saathos@unitedhealth.com    Saathos     Saathos                   14                   6                           12                         10                    48.00              1                          3                         1                       1          5              2            0                 0                2               0                    0               33.4                       1                        0                   1                    17                    0\n",
       "1   1365.0                   1    Alysanne Osgrey    alysanne.osgrey@ge.org   Alysanne      Osgrey                   87                   3                            8                          8                    40.35              1                          0                         0                       1          5              1           12                 0                2               0                    0               84.8                       1                        0                   3                    13                  170\n",
       "2    800.0                   1     Edwyd Fossoway    edwyd.fossoway@jnj.com      Edwyd    Fossoway                   15                   7                           11                          5                    19.77              1                          3                         0                       1          6              1            1                 0                1               0                    0               63.0                       1                        0                   2                    16                    0\n",
       "3    600.0                   1  Eleyna Westerling  eleyna.westerling@ge.org     Eleyna  Westerling                   13                   6                           11                          5                    90.00              1                          2                         0                       1          6              1           14                 0                3               0                    0               43.8                       1                        0                   2                    14                    0\n",
       "4   1490.0                   1      Elyn Norridge     elyn.norridge@jnj.com       Elyn    Norridge                   47                   8                            6                         10                    40.38              1                          0                         0                       0          5              1            5                 0                8               0                    0               84.8                       1                        1                   3                    12                  205"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "import pandas as pd # data science essentials\n",
    "import numpy as np #math esstentials \n",
    "import matplotlib.pyplot as plt # data visualization\n",
    "import seaborn as sns # enhanced data visualization\n",
    "import statsmodels.formula.api as smf # regression modeling\n",
    "from sklearn.model_selection import train_test_split # train/test split\n",
    "from sklearn.linear_model import LinearRegression #Linear Regression\n",
    "import sklearn.linear_model # linear models\n",
    "from sklearn.linear_model import Lasso  # Lasso Regression\n",
    "from sklearn.linear_model import ARDRegression # ARD Regression\n",
    "from sklearn.neighbors import KNeighborsRegressor # KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler # StandardScaler\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# importing file \n",
    "file = \"./Apprentice_Chef_Dataset.xlsx\"\n",
    "\n",
    "# reading file \n",
    "df_all = pd.read_excel(file)\n",
    "\n",
    "# lowercase column names to make typing in the future easier\n",
    "df_all.columns = map(str.lower, df_all.columns)\n",
    "\n",
    "# renaming mislabled \n",
    "df_all = df_all.rename(columns = {'largest_order_size':'avg_num_meals_customer'})\n",
    "\n",
    "\n",
    "# checking for nulls and imputing 0 \n",
    "#df_all.isnull().sum(axis = 0) \n",
    "df_all = df_all.fillna(0) #impute 0 for no data\n",
    "#df_all.isnull().sum(axis = 0) #check if removed\n",
    "\n",
    "# show head to verify renaming, loading of data and other transformations worked \n",
    "df_all.head(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Initial Exploration & Sample Log Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty exploration of all the variables \n",
    "# commented out as not needed \n",
    "\n",
    "#df_all.describe()\n",
    "\n",
    "# creating a new df excluding string cloumns \n",
    "#df =  df_all.loc[:, ~df_all.columns.isin([\"name\", \"email\", \"first_name\", \"family_name\"])]\n",
    "\n",
    "# loop to plot displots of all non-string columns\n",
    "#for i, column in enumerate(df.columns):\n",
    "   #sns.distplot(df[column]) \n",
    "   #plt.show()\n",
    "    \n",
    "# displot of revenue as example \n",
    "\n",
    "sns.distplot(a = df_all['revenue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# log transformation of revenue \n",
    "\n",
    "df_all['log_revenue'] = np.log10(df_all['revenue'])\n",
    "\n",
    "sns.distplot(a = df_all['log_revenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculated Features ##\n",
    "\n",
    "# total logins \n",
    "df_all['total_logins'] = df_all['pc_logins'] + df_all['mobile_logins']\n",
    "\n",
    "# total cancellations \n",
    "df_all['total_cancel'] = df_all['cancellations_before_noon'] + df_all['cancellations_after_noon']\n",
    "\n",
    "# every cancelled y/n\n",
    "\n",
    "df_all['every_cancelled'] = 0  #pre-allocating \n",
    "df_all.loc[(df_all['total_cancel']>0.5), 'every_cancelled' ] = 1\n",
    "\n",
    "# total orders without cancellations \n",
    "df_all['total_meals_ordered_without_cancel'] = df_all['total_meals_ordered'] - df_all['total_cancel']\n",
    "\n",
    "# order variety \n",
    "df_all['order_variety'] = df_all['total_meals_ordered'] / df_all['unique_meals_purch'] \n",
    "\n",
    "# amount of late to total \n",
    "df_all['late_ratio_total'] = df_all['late_deliveries'] / df_all['total_meals_ordered']\n",
    "\n",
    "# angry customer due to late meal\n",
    "df_all['contact_service_late'] = df_all['late_deliveries'] / df_all['contacts_w_customer_service'] \n",
    "\n",
    "# amount of customer service contact \n",
    "df_all['contact_service_orders'] = df_all['contacts_w_customer_service'] / df_all['total_meals_ordered']\n",
    "\n",
    "# logins to orders \n",
    "df_all['meals_per_login'] = df_all['total_meals_ordered'] / df_all['total_logins'] \n",
    "\n",
    "# clicks to order \n",
    "df_all['avg_num_meals_per_click'] = df_all['avg_num_meals_customer'] /df_all['avg_clicks_per_visit'] \n",
    "\n",
    "# clicks to order \n",
    "df_all['orders_per_login'] = df_all['total_meals_ordered'] / df_all['total_logins']\n",
    "\n",
    "# views to unique orders \n",
    "df_all['meals_per_pic_view'] = df_all['unique_meals_purch'] / df_all['total_photos_viewed'] \n",
    "\n",
    "# total lockers (and w/ y/n)\n",
    "df_all['total_lockers'] = df_all['package_locker'] + df_all['package_locker'] \n",
    "\n",
    "df_all['lockers_present'] = 0  #pre-allocating \n",
    "df_all.loc[(df_all['total_lockers']>0.5), 'lockers_present' ] = 1\n",
    "\n",
    "# order time change (and w/ y/n)\n",
    "df_all['order_time_change'] = df_all['early_deliveries'] + df_all['late_deliveries']\n",
    "\n",
    "df_all['order_time_change_ever'] = 0  #pre-allocating \n",
    "df_all.loc[(df_all['order_time_change']>0.5), 'order_time_change_ever' ] = 1\n",
    "\n",
    "# weekly plan y/n\n",
    "df_all['weekly_plan_present'] = 0  #pre-allocating \n",
    "df_all.loc[(df_all['weekly_plan']>0.5), 'weekly_plan_present' ] = 1\n",
    "\n",
    "# less than 30 order flag \n",
    "df_all['less_than_30_flag'] = 0  #pre-allocating \n",
    "df_all.loc[(df_all['weekly_plan']<30), 'less_than_30_flag' ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mail Feature ## \n",
    "\n",
    "# placeholder list\n",
    "placeholder_lst = []\n",
    "\n",
    "# looping over each email address\n",
    "for index, col in df_all.iterrows():\n",
    "    \n",
    "    # splitting email domain at '@'\n",
    "    split_email = df_all.loc[index, 'email'].split(sep = '@')\n",
    "    \n",
    "    # appending placeholder_lst with the results\n",
    "    placeholder_lst.append(split_email)\n",
    "    \n",
    "\n",
    "# converting placeholder_lst into a DataFrame \n",
    "email_df = pd.DataFrame(placeholder_lst)\n",
    "\n",
    "# renaming column to concatenate\n",
    "email_df.columns = ['0' , 'domain']\n",
    "\n",
    "# concatenating domain with df_all\n",
    "df_all = pd.concat([df_all, email_df['domain']],\n",
    "                     axis = 1)\n",
    "\n",
    "# creating column to be allocated with email type \n",
    "df_all['mail_type'] = '0'\n",
    "\n",
    "\n",
    "# defintion of email types \n",
    "\n",
    "prof = ['walmart.com','mmm.com', 'pg.com',\n",
    "        'amex.com', 'apple.com','boeing.com', \n",
    "        'caterpillar.com', 'chevron.com',\n",
    "        'cisco.com', 'cocacola.com', 'disney.com', \n",
    "        'dupont.com', 'exxon.com', 'ge.org', \n",
    "        'goldmansacs.com', 'homedepot.com', 'ibm.com', \n",
    "        'intel.com', 'jnj.com', 'jpmorgan.com', \n",
    "        'mcdonalds.com', 'merck.com', 'microsoft.com', \n",
    "        'nike.com', 'pfizer.com', \n",
    "        'travelers.com', 'unitedtech.com', 'unitedhealth.com',\n",
    "        'verizon.com', 'visa.com']\n",
    "\n",
    "pers = ['gmail.com','protonmail.com','yahoo.com','aol.com']\n",
    "\n",
    "spam = [\"hotmail.com\",\"me.com\", \"msn.com\",\"live.com\", \"passport.com\"]\n",
    "\n",
    "# loop for classification \n",
    "\n",
    "for index, row in df_all.iterrows():\n",
    "    if df_all.loc[index,'domain'] in prof: \n",
    "        df_all.loc[index,'mail_type'] = 'professional'\n",
    "        \n",
    "    elif df_all.loc[index,'domain'] in pers: \n",
    "        df_all.loc[index,'mail_type'] = 'personal'\n",
    "        \n",
    "    elif df_all.loc[index,'domain'] in spam: \n",
    "        df_all.loc[index,'mail_type'] = 'spam'\n",
    "        \n",
    "    elif df_all.loc[index, 'domain'] not in prof or df_all.loc[\n",
    "            row, 'domain'] not in pers or df_all.loc[row, 'domain'] not in spam:\n",
    "        df_all.loc[index, 'mail_type'] = 'unknown'\n",
    "        \n",
    "    else:\n",
    "        print('Mistake')\n",
    "    \n",
    "\n",
    "# creating dummies based on email type \n",
    "mail_dummies = pd.get_dummies(df_all.mail_type, prefix = \"dum_mail_type\",drop_first = True)\n",
    "\n",
    "# adding dummies to main dataframe \n",
    "df_all = pd.concat([df_all,mail_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummies ## \n",
    "\n",
    "# creating dummies based on rating\n",
    "rating_dummies = pd.get_dummies(df_all.median_meal_rating,prefix = \"dum_median_rating\",drop_first = True)\n",
    "\n",
    "# adding dummies to main dataframe \n",
    "df_all = pd.concat([df_all,rating_dummies], axis = 1)\n",
    "\n",
    "# creating dummies based on classes attended\n",
    "attended_dummies = pd.get_dummies(df_all.master_classes_attended,prefix = \"dum_master_classes_attended\", drop_first = True)\n",
    "\n",
    "# adding dummies to main dataframe \n",
    "df_all = pd.concat([df_all,attended_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-check distributions ## \n",
    "\n",
    "# creating a new df excluding string cloumns \n",
    "df =  df_all.loc[:, ~df_all.columns.isin([\"name\", \"email\", \"first_name\", \"family_name\", \"domain\", \"mail_type\"])]\n",
    "\n",
    "# loop to plot displots of all non-string columns\n",
    "#for i, column in enumerate(df.columns):\n",
    "   #sns.distplot(df[column]) \n",
    "   #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Log transformation ##\n",
    "\n",
    "# Changing 0 to 0.00000001 to avoid devide by zeros \n",
    "\n",
    "df_all['cancellations_before_noon'] = df_all['cancellations_before_noon'].replace(0,0.00000001)\n",
    "df_all['cancellations_after_noon'] = df_all['cancellations_after_noon'].replace(0,0.00000001)\n",
    "df_all['weekly_plan'] = df_all['weekly_plan'].replace(0,0.00000001)\n",
    "df_all['early_deliveries'] = df_all['early_deliveries'].replace(0,0.00000001)\n",
    "df_all['late_deliveries'] = df_all['late_deliveries'].replace(0,0.00000001)\n",
    "df_all['total_photos_viewed'] = df_all['total_photos_viewed'].replace(0,0.00000001)\n",
    "df_all['total_cancel'] = df_all['total_cancel'].replace(0,0.00000001)\n",
    "df_all['late_ratio_total'] = df_all['late_ratio_total'].replace(0,0.00000001)\n",
    "df_all['contact_service_late'] = df_all['contact_service_late'].replace(0,0.00000001)\n",
    "\n",
    "# log transformations\n",
    "df_all['log_total_meals_ordered'] = np.log10(df_all['total_meals_ordered'])\n",
    "df_all['log_unique_meals_purch'] = np.log10(df_all['unique_meals_purch'])\n",
    "df_all['log_contacts_w_customer_service'] = np.log10(df_all['contacts_w_customer_service'])\n",
    "df_all['log_product_categories_viewed'] = np.log10(df_all['product_categories_viewed'])\n",
    "df_all['log_avg_time_per_site_visit'] = np.log10(df_all['avg_time_per_site_visit'])\n",
    "df_all['log_cancellations_before_noon'] = np.log10(df_all['cancellations_before_noon'])\n",
    "df_all['log_cancellations_after_noon'] = np.log10(df_all['cancellations_after_noon'])\n",
    "df_all['log_weekly_plan'] = np.log10(df_all['weekly_plan'])\n",
    "df_all['log_early_deliveries'] = np.log10(df_all['early_deliveries'])\n",
    "df_all['log_late_deliveries'] = np.log10(df_all['late_deliveries'])\n",
    "df_all['log_avg_prep_vid_time'] = np.log10(df_all['avg_prep_vid_time'])\n",
    "df_all['log_avg_num_meals_customer'] = np.log10(df_all['avg_num_meals_customer'])\n",
    "df_all['log_median_meal_rating'] = np.log10(df_all['median_meal_rating'])\n",
    "df_all['log_total_photos_viewed'] = np.log10(df_all['total_photos_viewed'])\n",
    "df_all['log_total_logins'] = np.log10(df_all['total_logins'])\n",
    "df_all['log_total_cancel'] = np.log10(df_all['total_cancel'])\n",
    "df_all['log_total_meals_ordered_without_cancel'] = np.log10(df_all['total_meals_ordered_without_cancel'])\n",
    "df_all['log_order_variety'] = np.log10(df_all['order_variety'])\n",
    "df_all['log_late_ratio_total'] = np.log10(df_all['late_ratio_total'])\n",
    "df_all['log_contact_service_late'] = np.log10(df_all['contact_service_late'])\n",
    "df_all['log_contact_service_orders'] = np.log10(df_all['contact_service_orders'])\n",
    "df_all['log_avg_num_meals_per_click'] = np.log10(df_all['avg_num_meals_per_click'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Preparation and Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i) Removing columns and simple correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing columns not needed \n",
    "\n",
    "delete = [\"revenue\",\"name\", \"email\", \"first_name\", \"family_name\", \"domain\", \"mail_type\"]\n",
    "df = df_all.drop(delete, axis = 1)\n",
    "\n",
    "df_corr = df.corr(method = 'pearson')\n",
    "\n",
    "df_corr.loc[ : , \"log_revenue\"].round(decimals = 2).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii) X Variable Definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining all the x variables to avoid dublicates x and log \n",
    "\n",
    "x_variables = ['dum_master_classes_attended_1',\n",
    "'dum_master_classes_attended_2',\n",
    "'dum_master_classes_attended_3',\n",
    "'dum_median_rating_2',\n",
    "'dum_median_rating_3',\n",
    "'dum_median_rating_4',\n",
    "'dum_median_rating_5',\n",
    "'dum_mail_type_unknown',\n",
    "'dum_mail_type_spam',\n",
    "'dum_mail_type_professional',\n",
    "'log_total_meals_ordered',\n",
    "'log_total_meals_ordered_without_cancel',\n",
    "'log_avg_prep_vid_time',\n",
    "'log_order_variety',\n",
    "'log_median_meal_rating',\n",
    "'orders_per_login',\n",
    "'meals_per_login',\n",
    "'avg_num_meals_per_click',\n",
    "'order_variety',\n",
    "'log_avg_num_meals_customer',\n",
    "'total_photos_viewed',\n",
    "'log_avg_time_per_site_visit',\n",
    "'log_contacts_w_customer_service',\n",
    "'mobile_number',\n",
    "'product_categories_viewed',\n",
    "'log_cancellations_before_noon',\n",
    "'tastes_and_preferences',\n",
    "'pc_logins',\n",
    "'weekly_plan',\n",
    "'log_total_cancel',\n",
    "'cross_sell_success',\n",
    "'every_cancelled',\n",
    "'less_than_30_flag',\n",
    "'weekly_plan_present',\n",
    "'order_time_change_ever',\n",
    "'order_time_change',\n",
    "'total_lockers',\n",
    "'lockers_present',\n",
    "'log_early_deliveries',\n",
    "'refrigerated_locker',\n",
    "'mobile_logins',\n",
    "'package_locker',\n",
    "'unique_meals_purch',\n",
    "'contact_service_orders',\n",
    "'avg_clicks_per_visit']\n",
    "\n",
    "# printing for the plusses \n",
    "#for val in x_variables:\n",
    "    #print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii) Base Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a base model with a log transformed response variable\n",
    "\n",
    "\n",
    "# INSTANTIATING a model type \n",
    "lm_OLS = smf.ols(formula = \"\"\"log_revenue ~\n",
    "                                    dum_master_classes_attended_1 +\n",
    "dum_master_classes_attended_2 +\n",
    "dum_master_classes_attended_3 +\n",
    "dum_median_rating_2 +\n",
    "dum_median_rating_3 +\n",
    "dum_median_rating_4 +\n",
    "dum_median_rating_5 +\n",
    "dum_mail_type_unknown +\n",
    "dum_mail_type_spam +\n",
    "dum_mail_type_professional +\n",
    "log_total_meals_ordered +\n",
    "log_total_meals_ordered_without_cancel +\n",
    "log_avg_prep_vid_time +\n",
    "log_order_variety +\n",
    "log_median_meal_rating +\n",
    "orders_per_login +\n",
    "meals_per_login +\n",
    "avg_num_meals_per_click +\n",
    "order_variety +\n",
    "log_avg_num_meals_customer +\n",
    "total_photos_viewed +\n",
    "log_avg_time_per_site_visit +\n",
    "log_contacts_w_customer_service +\n",
    "mobile_number +\n",
    "product_categories_viewed +\n",
    "log_cancellations_before_noon +\n",
    "tastes_and_preferences +\n",
    "pc_logins +\n",
    "weekly_plan +\n",
    "log_total_cancel +\n",
    "cross_sell_success +\n",
    "every_cancelled +\n",
    "less_than_30_flag +\n",
    "weekly_plan_present +\n",
    "order_time_change_ever +\n",
    "order_time_change +\n",
    "total_lockers +\n",
    "lockers_present +\n",
    "log_early_deliveries +\n",
    "refrigerated_locker +\n",
    "mobile_logins +\n",
    "package_locker +\n",
    "unique_meals_purch +\n",
    "contact_service_orders +\n",
    "avg_clicks_per_visit\n",
    "\"\"\",data = df)\n",
    "\n",
    "# telling Python to FIT the data to the blueprint\n",
    "results = lm_OLS.fit()\n",
    "\n",
    "\n",
    "# printing a summary of the results\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iv) Final Variable Selection & Train/Test Splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new set of variables for OLS adjusted by p-value \n",
    "x_variables_OLS = ['dum_master_classes_attended_1',\n",
    "'dum_master_classes_attended_2',\n",
    "'dum_master_classes_attended_3',\n",
    "'dum_median_rating_2',\n",
    "'dum_median_rating_3',\n",
    "'dum_median_rating_4',\n",
    "'dum_median_rating_5',\n",
    "'dum_mail_type_unknown',\n",
    "'dum_mail_type_spam',\n",
    "'dum_mail_type_professional',\n",
    "'log_total_meals_ordered',\n",
    "'log_avg_prep_vid_time',\n",
    "'log_order_variety',\n",
    "'log_median_meal_rating',\n",
    "'log_avg_num_meals_customer',\n",
    "'total_photos_viewed',\n",
    "'log_contacts_w_customer_service',\n",
    "'cross_sell_success',\n",
    "'weekly_plan_present',\n",
    "'unique_meals_purch']\n",
    "\n",
    "#for val in x_variables_OLS:\n",
    "    #print(f\"{val} +\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple splits \n",
    "\n",
    "# Preparing a DataFrame based the the analysis above\n",
    "ols_data = df.loc[ : , x_variables_OLS] #for OLS p-value adjusted \n",
    "data = df.loc[ : , x_variables] #non OLS p-value adjusted \n",
    "\n",
    "\n",
    "# Preparing the target variable\n",
    "chef_target = df.loc[ : , 'log_revenue']\n",
    "\n",
    "# OLS p-value x-dataset (normal Y)\n",
    "x_train_OLS, x_test_OLS, y_train_OLS, y_test_OLS = train_test_split(\n",
    "            ols_data,         # x-variables\n",
    "            chef_target,   # y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "# Data without p value optimized \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "            data,         # x-variables\n",
    "            chef_target,   # y-variable\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n",
    "\n",
    "# Data for KNN \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            data,\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized split \n",
    "\n",
    "# copying df \n",
    "data_scale = data.copy()\n",
    "\n",
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with housing_data\n",
    "scaler.fit(data_scale)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(data_scale)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "X_scaled_df = pd.DataFrame(X_scaled)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_scaled_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardized train-test-split\n",
    "X_train_STAND, X_test_STAND, y_train_STAND, y_test_STAND = train_test_split(\n",
    "            X_scaled_df,\n",
    "            chef_target,\n",
    "            test_size = 0.25,\n",
    "            random_state = 219)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) OLS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model object\n",
    "lr = LinearRegression()\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "lr_fit = lr.fit(x_train_OLS, y_train_OLS)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "lr_pred = lr_fit.predict(x_test_OLS)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('OLS Training Score :', lr.score(x_train_OLS, y_train_OLS).round(4))\n",
    "print('OLS Testing Score  :', lr.score(x_test_OLS, y_test_OLS).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lr_train_score = lr.score(x_train_OLS, y_train_OLS).round(4) # using R-square\n",
    "lr_test_score  = lr.score(x_test_OLS, y_test_OLS).round(4)   # using R-square\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('OLS Train-Test Gap :', abs(lr_train_score - lr_test_score).round(4))\n",
    "lr_test_gap = abs(lr_train_score - lr_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "lr_model_values = zip(df[x_variables].columns,\n",
    "                      lr_fit.coef_.round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lr_model_lst = [('intercept', lr_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lr_model_values:\n",
    "    lr_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in lr_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Lasso Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model object\n",
    "lasso_model = sklearn.linear_model.Lasso(alpha = 0.001,\n",
    "                                         normalize = False) # default magitude\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "lasso_fit = lasso_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "lasso_pred = lasso_fit.predict(x_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('Lasso Training Score :', lasso_model.score(x_train, y_train).round(4))\n",
    "print('Lasso Testing Score  :', lasso_model.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "lasso_train_score = lasso_model.score(x_train, y_train).round(4) \n",
    "lasso_test_score  = lasso_model.score(x_test, y_test).round(4) \n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('Lasso Train-Test Gap :', abs(lasso_train_score - lasso_test_score).round(4))\n",
    "lasso_test_gap = abs(lasso_train_score - lasso_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "lasso_model_values = zip(df[x_variables].columns, lasso_fit.coef_.round(decimals = 2))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "lasso_model_lst = [('intercept', lasso_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in lasso_model_values:\n",
    "    lasso_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in lasso_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) ARD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model object\n",
    "ard_model = sklearn.linear_model.ARDRegression(normalize  = False)\n",
    "\n",
    "\n",
    "# FITTING the training data\n",
    "ard_fit = ard_model.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "ard_pred = ard_fit.predict(x_test)\n",
    "\n",
    "\n",
    "print('Training Score:', ard_model.score(x_train, y_train).round(4))\n",
    "print('Testing Score :',  ard_model.score(x_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "ard_train_score = ard_model.score(x_train, y_train).round(4)\n",
    "ard_test_score  = ard_model.score(x_test, y_test).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('ARD Train-Test Gap :', abs(ard_train_score - ard_test_score).round(4))\n",
    "ard_test_gap = abs(ard_train_score - ard_test_score).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipping each feature name to its coefficient\n",
    "ard_model_values = zip(df[x_variables].columns, ard_fit.coef_.round(decimals = 5))\n",
    "\n",
    "\n",
    "# setting up a placeholder list to store model features\n",
    "ard_model_lst = [('intercept', ard_fit.intercept_.round(decimals = 2))]\n",
    "\n",
    "\n",
    "# printing out each feature-coefficient pair one by one\n",
    "for val in ard_model_values:\n",
    "    ard_model_lst.append(val)\n",
    "    \n",
    "\n",
    "# checking the results\n",
    "for pair in ard_model_lst:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists for training set accuracy and test set accuracy\n",
    "training_accuracy = []\n",
    "test_accuracy     = []\n",
    "\n",
    "\n",
    "# building a visualization of 1 to 50 neighbors\n",
    "neighbors_settings = range(1, 21)\n",
    "\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # Building the model\n",
    "    clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Recording the training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    \n",
    "    # Recording the generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# plotting the visualization\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label = \"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the optimal number of neighbors\n",
    "opt_neighbors = test_accuracy.index(max(test_accuracy)) + 1\n",
    "print(f\"\"\"The optimal number of neighbors is {opt_neighbors}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a KNN model object\n",
    "knn_reg = KNeighborsRegressor(algorithm = 'auto',\n",
    "                              n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "knn_fit = knn_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING on new data\n",
    "knn_reg_pred = knn_fit.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Training Score:', knn_reg.score(X_train, y_train).round(4))\n",
    "print('KNN Testing Score :',  knn_reg.score(X_test, y_test).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_reg_score_train = knn_reg.score(X_train, y_train).round(4)\n",
    "knn_reg_score_test  = knn_reg.score(X_test, y_test).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap:', abs(knn_reg_score_train - knn_reg_score_test).round(4))\n",
    "knn_reg_test_gap = abs(knn_reg_score_train - knn_reg_score_test).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) KNN Scaled Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lists for training set accuracy and test set accuracy\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "\n",
    "# building a visualization of 1 to 50 neighbors\n",
    "neighbors_settings = range(1, 21)\n",
    "\n",
    "\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # Building the model\n",
    "    clf = KNeighborsRegressor(n_neighbors = n_neighbors)\n",
    "    clf.fit(X_train_STAND, y_train_STAND)\n",
    "    \n",
    "    # Recording the training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train_STAND, y_train_STAND))\n",
    "    \n",
    "    # Recording the generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test_STAND, y_test_STAND))\n",
    "\n",
    "\n",
    "# plotting the visualization\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(neighbors_settings, training_accuracy, label = \"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy,     label = \"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# finding the optimal number of neighbors\n",
    "opt_neighbors = test_accuracy.index(max(test_accuracy)) + 1\n",
    "print(f\"\"\"The optimal number of neighbors is {opt_neighbors}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a model with the optimal number of neighbors\n",
    "knn_stand = KNeighborsRegressor(algorithm = 'auto',\n",
    "                                n_neighbors = opt_neighbors)\n",
    "\n",
    "\n",
    "# FITTING the model based on the training data\n",
    "knn_stand_fit = knn_stand.fit(X_train_STAND, y_train_STAND)\n",
    "\n",
    "\n",
    "# PREDITCING on new data\n",
    "knn_stand_pred = knn_stand_fit.predict(X_test_STAND)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "print('KNN Training Score:', knn_stand.score(X_train_STAND, y_train_STAND).round(4))\n",
    "print('KNN Testing Score :',  knn_stand.score(X_test_STAND, y_test_STAND).round(4))\n",
    "\n",
    "\n",
    "# saving scoring data for future use\n",
    "knn_stand_score_train = knn_stand.score(X_train_STAND, y_train_STAND).round(4)\n",
    "knn_stand_score_test  = knn_stand.score(X_test_STAND, y_test_STAND).round(4)\n",
    "\n",
    "\n",
    "# displaying and saving the gap between training and testing\n",
    "print('KNN Train-Test Gap:', abs(knn_stand_score_train - knn_stand_score_test).round(4))\n",
    "knn_stand_test_gap = abs(knn_stand_score_train - knn_stand_score_test).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing results\n",
    "\n",
    "print(f\"\"\"\n",
    "Model      Train Score      Test Score      Gap\n",
    "-----      -----------      ----------      ----------\n",
    "OLS        {lr_train_score}           {lr_test_score}          {lr_test_gap}  \n",
    "Lasso      {lasso_train_score}           {lasso_test_score}          {lasso_test_gap}\n",
    "ARD        {ard_train_score}           {ard_test_score}          {ard_test_gap}\n",
    "KNN        {knn_reg_score_train}           {knn_reg_score_test}          {knn_reg_test_gap}\n",
    "KNN_stand {knn_stand_score_train}           {knn_stand_score_test}            {knn_stand_test_gap}\n",
    "\"\"\")\n",
    "\n",
    "NA = \"NA\" # defining NA for later\n",
    "\n",
    "# creating a dictionary for model results\n",
    "model_performance = {\n",
    "    \n",
    "    'Model Type'    : ['OLS', 'Lasso', 'ARD','KNN','KNN_stand'],\n",
    "           \n",
    "    'Training' : [lr_train_score, lasso_train_score,\n",
    "                                   ard_train_score,knn_reg_score_train,knn_stand_score_train],\n",
    "           \n",
    "    'Testing'  : [lr_test_score, lasso_test_score,\n",
    "                                   ard_test_score,knn_reg_score_test,knn_stand_score_test],\n",
    "                    \n",
    "    'Train-Test Gap' : [lr_test_gap, lasso_test_gap,\n",
    "                                        ard_test_gap,knn_reg_test_gap,knn_stand_test_gap],\n",
    "                    \n",
    "    'Model Size' : [len(lr_model_lst), len(lasso_model_lst),\n",
    "                                    len(ard_model_lst),NA,NA],\n",
    "                    \n",
    "    'Model' : [lr_model_lst, lasso_model_lst, ard_model_lst,NA,NA]}\n",
    "\n",
    "\n",
    "# converting model_performance into a DataFrame\n",
    "model_performance = pd.DataFrame(model_performance)\n",
    "\n",
    "# showing model results\n",
    "model_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<font color='red' size=\"100\"> Final Model: ARD </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font color='blue' size=\"70\"> The ARD Coefficients can be viewed below in detail </font>\n",
    "\n",
    "<br><hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display of ARD model coefficients\n",
    "ard_model_lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.667px",
    "left": "1301.67px",
    "right": "20px",
    "top": "125px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
