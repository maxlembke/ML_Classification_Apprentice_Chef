{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><b><font size = 6>A2: Unsupervised Analysis Project (Team)</f></b></u><br><br>\n",
    "<b><font size = 5> Machine Learning - DAT-5303</f></b>\n",
    "<br><br>\n",
    "<b>Authors</b>          : Eduardo Adami Kohn, Max Lembke, Rashmeet Kanwal <br>\n",
    "<b>Submission Date</b> : 22/Feb/2021<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Management Report: Personality type analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis is based on a survey conducted at Hult International Business School and for the sake of this analysis can be treated as representative. In total, 136 individuals answered questions pertaining to Goldberg’s version of the “Big Five” personality test, the “Hult DNA” (a personality assessment developed at the institution) and demographics (Hurtz & Donovan, 2000). The analysis processes this survey in order to produce actionable feedback for Apple marketing team in regards to customer acquisition and retention. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting an exploratory analysis of the data, several data cleaning and preparation steps were undertaken. First, the likert scale was flipped on negatively formulated questions, to make results easily comparable. In order to exclude unreliable responses, by individuals not dedicated to the survey due to time or other constraints, three cleaning steps were undertaken. \n",
    "\n",
    "First, respondents that answered with more than a 2-point scale difference in opposing questions were excluded (ex. likes attention vs. dislikes attention). Secondly, respondents with  more than a point difference on the duplicated questions were excluded. Finally, observations that consistently scored on the high or low end of the scale in either the Big Five, Hult DNA questions or both were discarded. This removed 31 observations leading to 105 observations after cleaning. \n",
    "\n",
    "After the cleaning steps, responses related to nationality were transformed to account for different spellings (ex. India vs Indian) and then divided into regions. Similarly, age buckets were formed for various age brackets. Furthermore, two features were engineered to highlight respondents that would be willing to change from any other device to mac and would consider changing to other devices from a mac (considering that all laptops cost the same). \n",
    "\n",
    "Finally, the score for each of the Big Five personality types were calculated using the established scoring system and the different traits of the Hult DNA were summed (Hasso, 2013). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis was conducted in two parts: The Big 5 Personality & Hult DNA. The core reason for this approach is that the two tests are designed to evaluated and scored separately and are conceptually independent of each other; following which will give a better understanding of how many clusters to work with for the given study. \n",
    "\n",
    "Moving further into the analysis, data was scaled to overcome overfitting and highlight the values that most affect the data or otherwise. It was found that neuroticism and extraversion dominate the other big 5 personality traits.\n",
    "\n",
    "The slope of the curve in the scree plot indicated the number of factors that should be used. The point where the slope of the curve plateaus indicates the number of factors to be generated. The first two, 0th and the 1st factors explain most of the variability, hence we are proceeding with analyzing the first two factors and dropping the rest.\n",
    "\n",
    "Based on the results from the scree plot, the two personalities, were derived:\n",
    "\n",
    "a) loyal_curious: This persona is the ideal colleague to have. They are open to new experiences, have a high mark on conscientiousness which makes them reliable at work and are extroverts which makes them open to communication, new people, ideas and perceptions (Hurtz & Donovan, 2000). \n",
    "They are also less neurotic which means they have the ability to acknowledge things that do not go the ideal way but also move on quickly to move forward (Hasso, 2013). Lastly, they are less agreeable, meaning they have an opinion, and they tend to stand by it. \n",
    "\n",
    "From a marketing perspective, this is an ideal target, as being less agreeable while being open to new experiences simultaneously provides the company with a unique opportunity.\n",
    "\n",
    "b) lonely_compulsive: This persona is a tough marketing target based on the Big 5 personalities. They are extremely high on neuroticism which means they can be very closed off to newer initiatives (Anglim & Grant, 2016). They are fairly low on extraversion and open to experiences and extremely low on conscientiousness and agreeableness as well. \n",
    "\n",
    "The combination of low agreeableness, open to experiences and high on neuroticism makes this persona less desirable to target due to their aversion of new experiences, but furthermore due to their “stubborness” they will prove to be a tough target to convert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hult DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Hult DNA, the required process of scaling the data to eliminate overfitting and find the variables that affected our data the most, was repeated. As per the results, dynamic thinking and inspiring productivity matter the most among the other variables. \n",
    "\n",
    "A similar approach as previously outlined was applied and it was decided to work on the first two factors as the variance between these two was the maximum.\n",
    "\n",
    "The basic objective of the Hult DNA analysis for persona building in addition to the Big 5 is to gauge how confident, influential and open to newness is the persona. The selection for this analysis is as follows:\n",
    "\n",
    "a) growth_mindset: This persona reflects high on dynamic thinking, listening carefully, resolving conflict, embracing change and inspiring productivity. Such a persona can be looked at as an influential employee within a business set- up and can be targeted through collective business development and marketing efforts. \n",
    "\n",
    "b) before_growth_mindset: This persona rates comparatively lower as compared to the growth mindset persona on all the features. Specifically, the negative markings on collaboration, presenting ideas and embracing change leads to this persona being comparatively unlikely to convert or influence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In clustering, the Big 5 & Hult DNA segments were combined (concatenated). The outcome of the scaling to this point aligns with our persona analysis thus far. Factors most influential are growth_mindset & loyal_curious.\n",
    "\n",
    "The dendrogram and inertia plot suggest three clusters as depicted in the graphs, basis which we found our clusters through the k- means model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Willing to change to Mac: Low hanging fruit </b>\n",
    "\n",
    "In the personas- before growth mindset, growth mindset, and loyal curious, targeting Cluster 1 is crucial as they are most likely to convert. While in the compulsive theorists segment, Cluster 2 is more likely to convert than Cluster 1.\n",
    "\n",
    "Overall, this leads to the preliminary conclusion that Cluster 1 is of greater interest. \n",
    "\n",
    "<b> Leaving Apple: Retention </b>\n",
    "\n",
    "Cluster 2, across all personas is likely to change to windows, hence marketing strategies to retain them should be considered. Cluster 0 & 1 are least likely to leave Apple and probably require less efforts to retain.\n",
    "\n",
    "<b> Region: Economic hubs </b>\n",
    "\n",
    "It can be observed that America, Europe, Asia and Africa hold most potential. Cluster 1 overall leads again, followed by Cluster 0 and lastly Cluster 2. This aligns with our analysis of clusters that will stick with Apple or otherwise.  \n",
    "\n",
    "<b> Age: When to target?</b>\n",
    "\n",
    "The analysis highlights particular focus on Cluster 1, and it can be suggested that one targets the potential consumers between 24- 27 years of age. This is the prime age for both students and young professionals. Targeting at this age will help spread the influence and hit the adaptability factor to an operating system that will help as the age progresses.\n",
    "\n",
    "<b> Gender: Who to Target?</b>\n",
    "\n",
    "In the gender analysis, yet again, Cluster 1 specifically males should be targeted.Interestingly, females in Cluster 0 specially in the ‘loyal curious persona’ are important to focus on for retention too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following recommendations serve two key purposes: Acquisition & Retention:\n",
    "\n",
    "• For new acquisitions, the focus should be Cluster 1 . Business development efforts for growth mindset & loyal curious personas should be centered around user generated content. They are curious to try out new ways of doing and to grow. New “ways of doing” especially on alternative i-devices such as the iPad Pro (as a mac replacement) could be highly appealing to this market (Lovejoy, 2019). Search engine marketing (SEM) is one of the highly recommended actions to take to acquire new customers and justify cost of acquisition.\n",
    "\n",
    "• For retention, Cluster 0  is the center of attention. Across personas, this cluster can be referred to  as “Apple Loyalists”. Such a segment is best targeted through influencers in their social and professional networks. Further Apple should continue it’s highly successful “Shot on iPhone” campaigns particularly incorporating users in the previously outlined Cluster, drafting them further into the “ecosystem” of Apple (Diaz, 2020).  \n",
    "\n",
    "• North America & Greater Middle east clearly are emerging markets based on the analysis. The age group of 24- 27 is of particular interest here and targeting this age group is highly crucial not only for new acquisitions but also due to their long-term retention potential. Considering this and the demographics, North America’s intake of international students is one of the highest. Hence, specialized campaigns targeting students is highly recommended. This could be focussed around the great gains of M1 Mac devices in areas such as Machine Learning (Bourke, 2020). Additionally, for the organic growth in these areas, a merge of BD & digital marketing efforts (SEO) would yield high returns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anglim, J., & Grant, S. (2016). Predicting Psychological and Subjective Well-Being from Personality: Incremental Prediction from 30 Facets Over the Big 5. Journal of Happiness Studies, 17(1), 59–80. https://doi.org/10.1007/s10902-014-9583-7\n",
    "\n",
    "Bourke, D. (2020, December 24). Apple’s New M1 Chip is a Machine Learning Beast. Medium. https://towardsdatascience.com/apples-new-m1-chip-is-a-machine-learning-beast-70ca8bfa6203\n",
    "\n",
    "Diaz, A.-C. (2020, August 18). Apple’s latest “Shot on iPhone” ad highlights the device’s editing capabilities. https://adage.com/creativity/work/apple-shot-and-edited-iphone/2274491\n",
    "\n",
    "Hasso, R. (2013). The impact of CEO’s personality traits (Big 5) and human resources management practices on the innovation performance in SMEs [Info:eu-repo/semantics/bachelorThesis]. University of Twente. http://essay.utwente.nl/63740/\n",
    "\n",
    "Hurtz, G. M., & Donovan, J. J. (2000). Personality and job performance: The Big Five revisited. - PsycNET. https://psycnet.apa.org/doiLanding?doi=10.1037%2F0021-9010.85.6.869\n",
    "\n",
    "Lovejoy, B. (2019, August 14). The iPad Pro is an increasingly viable Mac substitute for many—9to5Mac. https://9to5mac.com/2019/08/14/mac-substitute/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Loading Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import numpy as np                                      # mathematical essentials\n",
    "import pandas            as pd                          # data science essentials\n",
    "import matplotlib.pyplot as plt                         # fundamental data visualization\n",
    "import seaborn           as sns                         # enhanced visualizations\n",
    "from sklearn.preprocessing import StandardScaler        # standard scaler\n",
    "from sklearn.decomposition import PCA                   # pca\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage # dendrograms\n",
    "from sklearn.cluster         import KMeans              # k-means clustering\n",
    "\n",
    "# setting print options\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# specifying file name\n",
    "file = './survey_data.xlsx'\n",
    "\n",
    "# reading file into python\n",
    "df = pd.read_excel(io=file,\n",
    "                    sheet_name = 'raw',\n",
    "                    header = 0)\n",
    "\n",
    "# setting the ID as index\n",
    "df = df.set_index('surveyID')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## Helper functions \n",
    "\n",
    "########################################\n",
    "# scree_plot\n",
    "########################################\n",
    "\n",
    "def scree_plot(pca_object, export = False):\n",
    "    # building a scree plot\n",
    "\n",
    "    # setting plot size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    features = range(pca_object.n_components_)\n",
    "\n",
    "\n",
    "    # developing a scree plot\n",
    "    plt.plot(features,\n",
    "             pca_object.explained_variance_ratio_,\n",
    "             linewidth = 2,\n",
    "             marker = 'o',\n",
    "             markersize = 10,\n",
    "             markeredgecolor = 'black',\n",
    "             markerfacecolor = 'grey')\n",
    "\n",
    "\n",
    "    # setting more plot options\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.xticks(features)\n",
    "\n",
    "    if export == True:\n",
    "    \n",
    "        # exporting the plot\n",
    "        plt.savefig('./analysis_images/top_correlation_scree_plot.png')\n",
    "        \n",
    "    # displaying the plot\n",
    "    plt.show()\n",
    "\n",
    "########################################\n",
    "# inertia\n",
    "########################################\n",
    "def interia_plot(data, max_clust = 50):\n",
    "    \"\"\"\n",
    "PARAMETERS\n",
    "----------\n",
    "data      : DataFrame, data from which to build clusters. Dataset should be scaled\n",
    "max_clust : int, maximum of range for how many clusters to check interia, default 50\n",
    "    \"\"\"\n",
    "\n",
    "    ks = range(1, max_clust)\n",
    "    inertias = []\n",
    "\n",
    "\n",
    "    for k in ks:\n",
    "        # INSTANTIATING a kmeans object\n",
    "        model = KMeans(n_clusters = k)\n",
    "\n",
    "\n",
    "        # FITTING to the data\n",
    "        model.fit(data)\n",
    "\n",
    "\n",
    "        # append each inertia to the list of inertias\n",
    "        inertias.append(model.inertia_)\n",
    "\n",
    "\n",
    "\n",
    "    # plotting ks vs inertias\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    plt.plot(ks, inertias, '-o')\n",
    "\n",
    "\n",
    "    # labeling and displaying the plot\n",
    "    plt.xlabel('number of clusters, k')\n",
    "    plt.ylabel('inertia')\n",
    "    plt.xticks(ks)\n",
    "    plt.show()\n",
    "\n",
    "########################################\n",
    "# Upper limit (orignally developed by Clemens Weisgram)\n",
    "########################################\n",
    "\n",
    "def upper(dataframe, colname):\n",
    "    upper = np.percentile(dataframe[colname], 75) + 2.0 * (np.percentile(dataframe[colname], 75) - np.percentile(dataframe[colname], 25))\n",
    "    return upper\n",
    "\n",
    "\n",
    "########################################\n",
    "# Lower limit (orignally developed by Clemens Weisgram)\n",
    "########################################\n",
    "\n",
    "def lower(dataframe, colname):\n",
    "    lower = np.percentile(dataframe[colname], 75) - 2.0 * (np.percentile(dataframe[colname], 75) - np.percentile(dataframe[colname], 25))\n",
    "    return lower\n",
    "\n",
    "########################################\n",
    "# Boxplot\n",
    "########################################\n",
    "\n",
    "def boxplot(x, y, hue, data):\n",
    "    fig, ax = plt.subplots(figsize = (12, 8))\n",
    "    sns.boxplot(x = x,\n",
    "                y = y,\n",
    "                hue = hue,\n",
    "                data = data)\n",
    "\n",
    "\n",
    "    # formatting and displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# renaming all Columns to facilitate refrencing them\n",
    "df.rename(columns={'Am the life of the party':'party_life',\n",
    "'Feel little concern for others':'concern_for_others',\n",
    "'Am always prepared':'always_prepared',\n",
    "'Get stressed out easily':'stressed_easily',\n",
    "'Have a rich vocabulary':'rich_vocab',\n",
    "\"Don't talk a lot\":'not_talkative',\n",
    "'Am interested in people':'interested_in_people',\n",
    "'Leave my belongings around':'leave_belongings',\n",
    "'Am relaxed most of the time':'always_relaxed',\n",
    "'Have difficulty understanding abstract ideas':'difficulty_understanding_ideas',\n",
    "'Feel comfortable around people':'comfortable_with_people',\n",
    "'Insult people':'disrespectful',\n",
    "'Pay attention to details':'attention_to_details',\n",
    "'Worry about things':'worried_about_things',\n",
    "'Have a vivid imagination':'vivid_imagination',\n",
    "'Keep in the background':'keep_background',\n",
    "\"Sympathize with others' feelings\":'sympathize_others',\n",
    "'Make a mess of things':'messy',\n",
    "'Seldom feel blue':'not_usually_sad',\n",
    "'Am not interested in abstract ideas':'uninterested_abstract_ideas',\n",
    "'Start conversations':'starts_conversations',\n",
    "\"Am not interested in other people's problems\":'uninterested_others_problems',\n",
    "'Get chores done right away':'proactive',\n",
    "'Am easily disturbed':'distracted',\n",
    "'Have excellent ideas':'excellent_ideas',\n",
    "'Have little to say':'not_engaging',\n",
    "'Have a soft heart':'compassionate',\n",
    "'Often forget to put things back in their proper place':'forgetful_about_things',\n",
    "'Get upset easily':'upset_easily',\n",
    "'Do not have a good imagination':'good_imagination',\n",
    "'Talk to a lot of different people at parties':'engages_different_people',\n",
    "'Am not really interested in others':'uninterested_in_others',\n",
    "'Like order':'organised',\n",
    "'Change my mood a lot':'very_moody',\n",
    "'Am quick to understand things':'very_understanding',\n",
    "\"Don't like to draw attention to myself\":'dislikes_attention',\n",
    "'Take time out for others':'time_for_others',\n",
    "'Shirk my duties':'avoids_duty',\n",
    "'Have frequent mood swings':'frequent_moodswings',\n",
    "'Use difficult words':'difficult_words',\n",
    "\"Don't mind being the center of attention\":'like_attention_reversal', #change\n",
    "\"Feel others' emotions\":'compassionate_to_others',\n",
    "'Follow a schedule':'follows_schedules',\n",
    "'Get irritated easily':'irritated_quickly',\n",
    "'Spend time reflecting on things':'reflecting_on_things',\n",
    "'Am quiet around strangers':'quite_around_strangers',\n",
    "'Make people feel at ease':'comforts_people',\n",
    "'Am exacting in my work':'precision_in_work',\n",
    "'Often feel blue':'feels_sad',\n",
    "'Am full of ideas':'full_of_ideas',\n",
    "'See underlying patterns in complex situations':'investigates_complex_situations',\n",
    "\"Don't  generate ideas that are new and different\":'full_of_ideas_reversal', #change \n",
    "'Demonstrate an awareness of personal strengths and limitations':'self_aware',\n",
    "'Display a growth mindset':'growth_mindset',\n",
    "'Respond effectively to multiple priorities.1':'multitasking_1',\n",
    "\"Take initiative even when circumstances, objectives, or rules aren't clear.1\":'responds_unclear_objectives_1',\n",
    "'Encourage direct and open discussions.1':'promotes_open_discussions_1',\n",
    "'Respond effectively to multiple priorities':'multitasking_2',\n",
    "\"Take initiative even when circumstances, objectives, or rules aren't clear\":'responds_unclear_objectives_2',\n",
    "'Encourage direct and open discussions':'promotes_open_discussions_2',\n",
    "'Listen carefully to others':'good_listener',\n",
    "\"Don't persuasively sell a vision or idea\":'nonpersuasive_vision',\n",
    "'Build cooperative relationships':'builds_relationships',\n",
    "'Work well with people from diverse cultural backgrounds':'multicultural_behaviour',\n",
    "'Effectively negotiate interests, resources, and roles':'good_negotiation',\n",
    "\"Can't rally people on the team around a common goal\":'lacks_leadership',\n",
    "'Translate ideas into plans that are organized and realistic':'executes_plans',\n",
    "'Resolve conflicts constructively':'good_problem_solver',\n",
    "'Seek and use feedback from teammates':'open_to_feedback',\n",
    "'Coach teammates for performance and growth':'coach_teammates',\n",
    "'Drive for results':'results_driven',\n",
    "'What laptop do you currently have?':'current_laptop',\n",
    "'What laptop would you buy in next assuming if all laptops cost the same?':'next_purchase',\n",
    "'What program are you in?':'program',\n",
    "'What is your age?':'age',\n",
    "'Gender':'gender',\n",
    "'What is your nationality? ':'nationality',\n",
    "'What is your ethnicity?':'ethnicity'},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale Flip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing the scale on negative questions to allow for comparission \n",
    "# defining columns that have a reverse scale and filtering on these \n",
    "filters = df.filter([\"concern_for_others\", \"stressed_easily\", \"not_talkative\",\n",
    "    \"leave_belongings\", \"difficulty_understanding_ideas\",\n",
    "      \"disrespectful\", \"worried_about_things\", \"keep_background\", \"messy\",\n",
    "      \"uninterested_abstract_ideas\", \"uninterested_others_problems\",\n",
    "      \"distracted\", \"not_engaging\", \"forgetful_about_things\",\n",
    "      \"upset_easily\", \"good_imagination\", \"uninterested_in_others\",\n",
    "      \"very_moody\", \"dislikes_attention\", \"avoids_duty\",\n",
    "      \"frequent_moodswings\", \"irritated_quickly\", \"quite_around_strangers\",\n",
    "      \"feels_sad\", \"full_of_ideas_reversal\", \n",
    "      \"nonpersuasive_vision\", \"lacks_leadership\"])\n",
    "\n",
    "# grabbing the column names of these \n",
    "to_be_flipped = filters.columns\n",
    "\n",
    "# changing the scale \n",
    "for i in to_be_flipped:\n",
    "    for index, value in df.iterrows():\n",
    "        df.loc[index, i ] = (6 - df.loc[index, i]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping possible misleading observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setup of exclusion list \n",
    "\n",
    "# creating an empty list to append to\n",
    "# this list will collect all the IDs that are misleading\n",
    "ind = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6502277e909e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not iterable"
     ]
    }
   ],
   "source": [
    "ind = []\n",
    "\n",
    "ind.extend(().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion similiar question responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## excluding observations that answered differently to the same question \n",
    "## if the difference is more than one point it is excluded \n",
    "\n",
    "# calculating the difference \n",
    "diff_ppl = abs(df.loc[:,\"interested_in_people\"] - df.loc[:,\"uninterested_in_others\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 2\n",
    "ind.extend(diff_ppl [diff_ppl  > 2].index)\n",
    "\n",
    "# calculating the difference \n",
    "diff_relaxed = abs(df.loc[:,\"always_relaxed\"] - df.loc[:,\"stressed_easily\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 2\n",
    "ind.extend(diff_relaxed[diff_relaxed > 2].index)\n",
    "\n",
    "# calculating the difference \n",
    "diff_attention = abs(df.loc[:,\"like_attention_reversal\"] - df.loc[:,\"dislikes_attention\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 2\n",
    "ind.extend(diff_attention[diff_attention > 2].index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Exclusion double question responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## excluding observations that answered differently to the same question \n",
    "## if the difference is more than one point it is excluded \n",
    "\n",
    "# calculating the difference \n",
    "diff_obj = abs(df.loc[:,\"responds_unclear_objectives_2\"] - df.loc[:,\"responds_unclear_objectives_1\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 1 \n",
    "ind.extend(diff_obj [diff_obj  > 1].index)\n",
    "\n",
    "# calculating the difference \n",
    "diff_discussion = abs(df.loc[:,\"promotes_open_discussions_2\"] - df.loc[:,\"promotes_open_discussions_1\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 1 \n",
    "ind.extend(diff_discussion[diff_discussion > 1].index)\n",
    "\n",
    "# calculating the difference \n",
    "diff_priorities = abs(df.loc[:,\"multitasking_2\"] - df.loc[:,\"multitasking_1\"])\n",
    "\n",
    "# appending list with id if the difference is greater than 1 \n",
    "ind.extend(diff_priorities[diff_priorities > 1].index)\n",
    "\n",
    "# deletion of dublicate columns\n",
    "delete = [\"responds_unclear_objectives_2\",\"promotes_open_discussions_2\",\"multitasking_2\"]\n",
    "df = df.drop(delete, axis = 1)\n",
    "\n",
    "# renaming of remaining column \n",
    "df.rename(columns={\"responds_unclear_objectives_1\":\"responds_unclear_objectives\",\n",
    "                  \"promotes_open_discussions_1\":\"promotes_open_discussions\",\n",
    "                  \"multitasking_1\":\"multitasking\"},\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclusion \"undedicated\" people "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exclusing observations that answer many questions with similiar ratings \n",
    "## this is done for the big5, hult dna and both \n",
    "\n",
    "# defining hult dna\n",
    "all_big5 = ['excellent_ideas', 'very_understanding','difficult_words','full_of_ideas',\n",
    "'uninterested_abstract_ideas','good_imagination','difficulty_understanding_ideas',\n",
    "'rich_vocab','vivid_imagination','reflecting_on_things','party_life','like_attention_reversal',\n",
    "'comfortable_with_people','starts_conversations','engages_different_people','not_talkative',\n",
    "'full_of_ideas_reversal','quite_around_strangers','keep_background','not_engaging','always_prepared',\n",
    "'attention_to_details','proactive','follows_schedules','precision_in_work',\n",
    "'leave_belongings','forgetful_about_things','avoids_duty','messy','organised','interested_in_people',\n",
    "'sympathize_others','compassionate','time_for_others','compassionate_to_others',\n",
    "'comforts_people','uninterested_in_others','disrespectful','uninterested_others_problems',\n",
    "'concern_for_others','irritated_quickly','stressed_easily','upset_easily','frequent_moodswings',\n",
    "'worried_about_things','always_relaxed','not_usually_sad','feels_sad','distracted',\n",
    "'very_moody']\n",
    "\n",
    "# calculating avg for observation\n",
    "df['avg_big5'] = df[all_big5].mean(axis=1)\n",
    "\n",
    "# exclusing observations on the lower and upper end of the scale \n",
    "ind.extend(df[\"avg_big5\"][df[\"avg_big5\"] > upper(df, \"avg_big5\")].index)\n",
    "ind.extend(df[\"avg_big5\"][df[\"avg_big5\"] < lower(df, \"avg_big5\")].index)\n",
    "\n",
    "all_hultdna = ['self_aware', 'open_to_feedback','good_listener','executes_plans','builds_relationships', \n",
    "               'lacks_leadership','multicultural_behaviour', 'lacks_leadership','coach_teammates', \n",
    "               'good_negotiation','full_of_ideas_reversal', 'responds_unclear_objectives',\n",
    "               'results_driven','investigates_complex_situations','multitasking',\n",
    "               'executes_plans','nonpersuasive_vision', 'good_negotiation',\n",
    "               'promotes_open_discussions', 'good_problem_solver']\n",
    "\n",
    "df['avg_hultdna'] = df[all_hultdna].mean(axis=1)\n",
    "\n",
    "ind.extend(df[\"avg_hultdna\"][df[\"avg_hultdna\"] > upper(df, \"avg_hultdna\")].index)\n",
    "ind.extend(df[\"avg_hultdna\"][df[\"avg_hultdna\"] < lower(df, \"avg_hultdna\")].index)\n",
    "\n",
    "df['both_avg'] = ((df['avg_hultdna']+df['avg_big5'])/2)\n",
    "\n",
    "ind.extend(df[\"both_avg\"][df[\"both_avg\"] > upper(df, \"both_avg\")].index)\n",
    "ind.extend(df[\"both_avg\"][df[\"both_avg\"] < lower(df, \"both_avg\")].index)\n",
    "\n",
    "delete = ['both_avg','avg_hultdna','avg_big5']\n",
    "\n",
    "df = df.drop(delete, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Removing the misleading observations \n",
    "\n",
    "df = df.drop((pd.unique(ind).tolist()), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.unique(ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Cleaning of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## making all other observation values in text fields lower case\n",
    "\n",
    "# defining columns \n",
    "cols = [\"gender\",\"ethnicity\",\"program\",\"next_purchase\",\"current_laptop\",\"nationality\"]\n",
    "\n",
    "# looping over and changing\n",
    "for element in cols:\n",
    "    df.loc[:,element] = df.loc[:,element].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organising Nationality Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# fixing nationalities\n",
    "df['nationality'] = df['nationality'].replace('brazil', 'brazilian')\n",
    "df['nationality'] = df['nationality'].replace('colombia', 'colombian')\n",
    "df['nationality'] = df['nationality'].replace('czech republic', 'czech')\n",
    "df['nationality'] = df['nationality'].replace('ecuador', 'ecuadorian')\n",
    "df['nationality'] = df['nationality'].replace('germany', 'german')\n",
    "df['nationality'] = df['nationality'].replace('indonesia', 'indonesian')\n",
    "df['nationality'] = df['nationality'].replace('nigeria', 'nigerian')\n",
    "df['nationality'] = df['nationality'].replace('peru', 'peruvian')\n",
    "df['nationality'] = df['nationality'].replace('russia', 'russian')\n",
    "df['nationality'] = df['nationality'].replace('spain', 'spanish')\n",
    "df['nationality'] = df['nationality'].replace('canada', 'canadian')\n",
    "df['nationality'] = df['nationality'].replace('china', 'chinese')\n",
    "df['nationality'] = df['nationality'].replace('usa', 'american')\n",
    "df['nationality'] = df['nationality'].replace('panama', 'panamanian')\n",
    "df['nationality'] = df['nationality'].replace('taiwan', 'taiwanese')\n",
    "df['nationality'] = df['nationality'].replace('republic of korea', 'south korean')\n",
    "df['nationality'] = df['nationality'].replace('costarrican', 'costa rican')\n",
    "df['nationality'] = df['nationality'].replace('korea', 'south korean')\n",
    "df['nationality'] = df['nationality'].replace('japan', 'japanese')\n",
    "df['nationality'] = df['nationality'].replace('mauritius', 'mauritian')\n",
    "df['nationality'] = df['nationality'].replace('philippines', 'filipino')\n",
    "df['nationality'] = df['nationality'].replace('south korea', 'south korean')\n",
    "\n",
    "# fixing punctuation\n",
    "df['nationality'] = df['nationality'].replace('indian.', 'indian') #removing full-stop\n",
    "df['nationality'] = df['nationality'].replace('dominican ', 'dominican')# space after\n",
    "df['nationality'] = df['nationality'].replace('congolese (dr congo)', 'congolese') #removing brackets\n",
    "\n",
    "# aligning dual nationality holders into a single format\n",
    "df['nationality'] = df['nationality'].replace('british, indian', 'british/indian')\n",
    "df['nationality'] = df['nationality'].replace('italian and spanish', 'italian/spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continent Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping countries to key regions\n",
    "\n",
    "#Pre-allocating the column \n",
    "df[\"region\"] = \"\"\n",
    "\n",
    "# country by economic regions\n",
    "south_america = ['venezuelan', 'peruvian', 'brazilian','colombian','ecuadorian',]\n",
    "\n",
    "north_america = ['american', 'canadian']\n",
    "\n",
    "europe = ['german', 'russian', 'italian', 'norwegian', 'czech','spanish', 'swiss','belarus','ukrainian','british','belgian','portuguese']\n",
    "\n",
    "africa = ['nigerian', 'congolese','kenyan', 'mauritian','ghanaian','ugandan']\n",
    "\n",
    "asia = ['indian', 'chinese', 'indonesian', 'taiwanese','south korean', 'thai','vietnamese','filipino','japanese']\n",
    "\n",
    "greater_middle_east = ['turkish', 'kyrgyz','pakistani']\n",
    "\n",
    "central_america = ['dominican', 'mexican','panamanian','costa rican']\n",
    "\n",
    "not_answered = ['italian/spanish', 'prefer not to answer', 'german/american','british/indian']\n",
    "\n",
    "\n",
    "for row in df.index:\n",
    "    if df.loc[row,\"nationality\"] in south_america:\n",
    "        df.loc[row,\"region\"] = \"South_America\"\n",
    "   \n",
    "    elif df.loc[row,\"nationality\"] in north_america:\n",
    "        df.loc[row,\"region\"] = \"North_america\"\n",
    "    \n",
    "    elif df.loc[row,\"nationality\"] in europe:\n",
    "        df.loc[row,\"region\"] = \"Europe\"\n",
    "        \n",
    "    elif df.loc[row,\"nationality\"] in africa:\n",
    "        df.loc[row,\"region\"] = \"Africa\"\n",
    "    \n",
    "    elif df.loc[row,\"nationality\"] in asia:\n",
    "        df.loc[row,\"region\"] = \"Asia\"\n",
    "        \n",
    "    elif df.loc[row,\"nationality\"] in greater_middle_east:\n",
    "        df.loc[row,\"region\"] = \"Greater_middle_east\"\n",
    "        \n",
    "    elif df.loc[row,\"nationality\"] in central_america:\n",
    "        df.loc[row,\"region\"] = \"Central_america\"\n",
    "    else:\n",
    "        df.loc[row,\"region\"] = \"not_answered\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Age Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## building a feature with different age groups \n",
    "#Pre-allocating the column \n",
    "df[\"age_group\"] = \"\"\n",
    "\n",
    "# looping over the df\n",
    "for row in df.index:\n",
    "    if df.loc[row,\"age\"] <= 23:\n",
    "        df.loc[row,\"age_group\"] = \"21-23\"\n",
    "    \n",
    "    elif df.loc[row,\"age\"] >= 24 and df.loc[row,\"age\"] <= 27:\n",
    "        df.loc[row,\"age_group\"] = \"24-27\"\n",
    "    \n",
    "    elif df.loc[row,\"age\"] >= 28 and df.loc[row,\"age\"] <= 34:\n",
    "        df.loc[row,\"age_group\"] = \"28-34\"\n",
    "    \n",
    "    else: \n",
    "        df.loc[row,\"age_group\"] = \"34+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wiling to change feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## building a feature of people willing to switch to mac \n",
    "\n",
    "#Pre-allocating the column \n",
    "df[\"willing_to_change_to_mac\"] = \"\"\n",
    "\n",
    "# looping over the df\n",
    "for row in df.index:\n",
    "    if df.loc[row,\"current_laptop\"] != \"macbook\" and df.loc[row,\"next_purchase\"] == \"macbook\":\n",
    "        df.loc[row,\"willing_to_change_to_mac\"] = 1\n",
    "    else:\n",
    "        df.loc[row,\"willing_to_change_to_mac\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to leave Apple Universe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## building a feature of people willing to switch to mac \n",
    "\n",
    "#Pre-allocating the column \n",
    "df[\"leaving_apple\"] = \"\"\n",
    "\n",
    "# looping over the df\n",
    "for row in df.index:\n",
    "    if df.loc[row,\"current_laptop\"] == \"macbook\" and df.loc[row,\"next_purchase\"] != \"macbook\":\n",
    "        df.loc[row,\"leaving_apple\"] = 1\n",
    "    else:\n",
    "        df.loc[row,\"leaving_apple\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Big 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperating the Big 5 and summing in new df \n",
    "\n",
    "# making copy of df \n",
    "big5 = df.copy()\n",
    "\n",
    "# defining elements \n",
    "openness_to_experiences = ['excellent_ideas',\n",
    "'very_understanding',\n",
    "'difficult_words',\n",
    "'full_of_ideas',\n",
    "'uninterested_abstract_ideas',\n",
    "'good_imagination',\n",
    "'difficulty_understanding_ideas',\n",
    "'rich_vocab',\n",
    "'vivid_imagination',\n",
    "'reflecting_on_things']\n",
    "\n",
    "# summing \n",
    "big5['openness_to_experiences'] = 8 + (big5[openness_to_experiences].sum(axis=1))\n",
    "\n",
    "# defining elements \n",
    "extraversion = ['party_life',\n",
    "'full_of_ideas_reversal',\n",
    "'comfortable_with_people',\n",
    "'starts_conversations',\n",
    "'engages_different_people',\n",
    "'not_talkative',\n",
    "'like_attention_reversal',\n",
    "'quite_around_strangers',\n",
    "'keep_background',\n",
    "'not_engaging']\n",
    "\n",
    "# summing \n",
    "big5['extraversion'] = 20 + (big5[extraversion].sum(axis=1))\n",
    "\n",
    "# defining elements \n",
    "conscientiousness = ['always_prepared',\n",
    "'attention_to_details',\n",
    "'proactive',\n",
    "'follows_schedules',\n",
    "'precision_in_work',\n",
    "'leave_belongings',\n",
    "'forgetful_about_things',\n",
    "'avoids_duty',\n",
    "'messy',\n",
    "'organised']\n",
    "\n",
    "# summing \n",
    "big5['conscientiousness'] = 14 + (big5[conscientiousness].sum(axis=1))\n",
    "\n",
    "# defining elements \n",
    "agreeableness = ['interested_in_people',\n",
    "'sympathize_others',\n",
    "'compassionate',\n",
    "'time_for_others',\n",
    "'compassionate_to_others',\n",
    "'comforts_people',\n",
    "'uninterested_in_others',\n",
    "'disrespectful',\n",
    "'uninterested_others_problems',\n",
    "'concern_for_others']\n",
    "\n",
    "# summing \n",
    "big5['agreeableness'] = 14 - (big5[agreeableness].sum(axis=1))\n",
    "\n",
    "# defining elements \n",
    "neuroticism = ['irritated_quickly',\n",
    "'stressed_easily',\n",
    "'upset_easily',\n",
    "'frequent_moodswings',\n",
    "'worried_about_things',\n",
    "'always_relaxed',\n",
    "'not_usually_sad',\n",
    "'feels_sad',\n",
    "'distracted',\n",
    "'very_moody']\n",
    "\n",
    "# summing \n",
    "big5['neuroticism'] = 38 - (big5[neuroticism].sum(axis=1))\n",
    "\n",
    "# creating new df \n",
    "df_big5 = big5[[\"openness_to_experiences\",\"neuroticism\",\"agreeableness\",\"conscientiousness\",\"extraversion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_big5.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Hult DNA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperating the Hult DNA elements and summing in new df \n",
    "\n",
    "# making copy of df \n",
    "hultdna = df.copy()\n",
    "\n",
    "# defining elements & summing \n",
    "self_awareness = ['self_aware', 'open_to_feedback']\n",
    "hultdna['self_awareness'] = hultdna[self_awareness].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "speaks_listen_carefully = ['good_listener','executes_plans']\n",
    "hultdna['speaks_listen_carefully'] = hultdna[speaks_listen_carefully].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "collab_relationships = ['builds_relationships', 'lacks_leadership']\n",
    "hultdna['collab_relationships'] = hultdna[collab_relationships].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "embraces_change = ['multicultural_behaviour', 'lacks_leadership']\n",
    "hultdna['embraces_change'] = hultdna[embraces_change].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "influences_confidently = ['coach_teammates', 'good_negotiation']\n",
    "hultdna['influences_confidently'] = hultdna[influences_confidently].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "inspire_productivity = ['full_of_ideas_reversal',\n",
    "                       'responds_unclear_objectives', \n",
    "                       'results_driven']\n",
    "\n",
    "# defining elements & summing \n",
    "hultdna['inspire_productivity'] = hultdna[inspire_productivity].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "dynamic_thinking = ['investigates_complex_situations',\n",
    "                   'multitasking', \n",
    "                   'executes_plans']\n",
    "hultdna['dynamic_thinking'] = hultdna[dynamic_thinking].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "presents_ideas = ['nonpersuasive_vision', 'good_negotiation']\n",
    "hultdna['presents_ideas'] = hultdna[presents_ideas].sum(axis=1)\n",
    "\n",
    "# defining elements & summing \n",
    "resolves_conflicts = ['promotes_open_discussions', 'good_problem_solver']\n",
    "hultdna['resolves_conflicts'] = hultdna[resolves_conflicts].sum(axis=1)\n",
    "\n",
    "# creating new df \n",
    "df_hultdna = hultdna[[\"resolves_conflicts\",\"presents_ideas\",\"dynamic_thinking\",\"inspire_productivity\",\n",
    "                      \"influences_confidently\",\"embraces_change\",\"collab_relationships\",\"speaks_listen_carefully\",\n",
    "                     \"self_awareness\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hultdna.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperating Demographic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seperating the demographic elements into a new df \n",
    "demo_df = df.copy()\n",
    "\n",
    "demo_df = df.filter([\"current_laptop\",\"next_purchase\",\"program\",\"nationality\",\n",
    "                     \"age_group\",\"willing_to_change_to_mac\",\"leaving_apple\",\"region\",\"gender\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Big 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with the data\n",
    "scaler.fit(df_big5)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(df_big5)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "df_big5_scaled = pd.DataFrame(X_scaled)\n",
    "\n",
    "\n",
    "# reattaching column names\n",
    "df_big5_scaled.columns = df_big5.columns\n",
    "\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(np.var(df_big5), '\\n\\n')\n",
    "print(np.var(df_big5_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plotting & Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = PCA(n_components = None,\n",
    "          random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "big5_pca = pca.fit_transform(df_big5_scaled)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Principal Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a new model using the first three principal components\n",
    "pca_2 = PCA(n_components = 2,\n",
    "            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_pca_2 = pca_2.fit_transform(df_big5_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of unlimited and reduced PCA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Max PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = MAX)\n",
    "factor_loadings = pd.DataFrame(pd.np.transpose(pca.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings = factor_loadings.set_index(df_big5_scaled.columns)\n",
    "\n",
    "\n",
    "##################\n",
    "### 3 PC Model ###\n",
    "##################\n",
    "# transposing pca components (pc = 3)\n",
    "factor_loadings_2 = pd.DataFrame(pd.np.transpose(pca_2.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_2 = factor_loadings_2.set_index(df_big5_scaled.columns)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(f\"\"\"\n",
    "MAX Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings.round(2)}\n",
    "\n",
    "\n",
    "2 Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings_2.round(2)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Results and defining segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_2.columns = ['loyal_curious',\n",
    "                             'compulsive_theorist'] \n",
    "\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis each factor loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing factor strengths per customer\n",
    "X_pca_reduced = pca_2.transform(df_big5_scaled)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_pca_df = pd.DataFrame(X_pca_reduced)\n",
    "\n",
    "\n",
    "# renaming columns\n",
    "X_pca_df.columns = factor_loadings_2.columns\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to excel\n",
    "X_pca_df.to_excel('./PCA Factor Loadings Big5.xlsx',\n",
    "                  index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA: Hult DNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with the data\n",
    "scaler.fit(df_hultdna)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled = scaler.transform(df_hultdna)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "df_hultdna_scaled = pd.DataFrame(X_scaled)\n",
    "\n",
    "\n",
    "# reattaching column names\n",
    "df_hultdna_scaled.columns = df_hultdna.columns\n",
    "\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(np.var(df_hultdna), '\\n\\n')\n",
    "print(np.var(df_hultdna_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree Plotting & Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = PCA(n_components = None,\n",
    "          random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "hultdna_pca = pca.fit_transform(df_hultdna_scaled)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Principal Components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a new model using the first three principal components\n",
    "pca_2 = PCA(n_components = 2,\n",
    "            random_state = 219)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_pca_2 = pca_2.fit_transform(df_hultdna_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of unlimited and reduced PCA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "### Max PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = MAX)\n",
    "factor_loadings = pd.DataFrame(pd.np.transpose(pca.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings = factor_loadings.set_index(df_hultdna_scaled.columns)\n",
    "\n",
    "\n",
    "##################\n",
    "### 3 PC Model ###\n",
    "##################\n",
    "# transposing pca components (pc = 3)\n",
    "factor_loadings_2 = pd.DataFrame(pd.np.transpose(pca_2.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_2 = factor_loadings_2.set_index(df_hultdna_scaled.columns)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(f\"\"\"\n",
    "MAX Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings.round(2)}\n",
    "\n",
    "\n",
    "2 Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings_2.round(2)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Results and defining segments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_2.columns = ['growth_mindset',\n",
    "                             'before_growth_mindset'] \n",
    "\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis each factor loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzing factor strengths per customer\n",
    "X_pca_reduced = pca_2.transform(df_hultdna_scaled)\n",
    "\n",
    "\n",
    "# converting to a DataFrame\n",
    "X_pca_df = pd.DataFrame(X_pca_reduced)\n",
    "\n",
    "\n",
    "# renaming columns\n",
    "X_pca_df.columns = factor_loadings_2.columns\n",
    "\n",
    "\n",
    "# checking the results\n",
    "X_pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to excel\n",
    "X_pca_df.to_excel('./PCA Factor Loadings Hultdna.xlsx',\n",
    "                  index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing both excel files \n",
    "\n",
    "# Big 5\n",
    "# specifying file name\n",
    "file_big5 = './PCA Factor Loadings Big5.xlsx'\n",
    "\n",
    "# reading file into python\n",
    "df_big5_factors = pd.read_excel(io=file_big5,\n",
    "                    header = 0)\n",
    "\n",
    "# Hult DNA\n",
    "# specifying file name\n",
    "file_hultdna = './PCA Factor Loadings Hultdna.xlsx'\n",
    "\n",
    "# reading file into python\n",
    "df_hultdna_factors = pd.read_excel(io=file_hultdna,\n",
    "                    header = 0)\n",
    "\n",
    "# merging both df on index\n",
    "\n",
    "all_data = pd.concat([df_hultdna_factors,df_big5_factors],\n",
    "                                axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a StandardScaler() object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "# FITTING the scaler with the data\n",
    "scaler.fit(all_data)\n",
    "\n",
    "\n",
    "# TRANSFORMING our data after fit\n",
    "X_scaled_pca = scaler.transform(all_data)\n",
    "\n",
    "\n",
    "# converting scaled data into a DataFrame\n",
    "pca_scaled = pd.DataFrame(X_scaled_pca)\n",
    "\n",
    "\n",
    "# reattaching column names\n",
    "pca_scaled.columns = ['growth_mindset', \n",
    "                      'before_growth_mindset',\n",
    "                      'loyal_curious',\n",
    "                     'lonely_compulsive'] \n",
    "\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(pd.np.var(all_data), '\\n\\n')\n",
    "print(pd.np.var(pca_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram & Inertia plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping data based on Ward distance\n",
    "standard_mergings_ward = linkage(y = pca_scaled,\n",
    "                                 method = 'ward',\n",
    "                                 optimal_ordering = True)\n",
    "\n",
    "\n",
    "# setting plot size\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "# developing a dendrogram\n",
    "dendrogram(Z = standard_mergings_ward,\n",
    "           leaf_rotation = 90,\n",
    "           leaf_font_size = 6)\n",
    "\n",
    "# show plot \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the inertia_plot() function\n",
    "interia_plot(data = pca_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing k-Means Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATING a k-Means object with clusters\n",
    "customers_k_pca = KMeans(n_clusters   = 3,\n",
    "                         random_state = 219)\n",
    "\n",
    "\n",
    "# fitting the object to the data\n",
    "customers_k_pca.fit(pca_scaled)\n",
    "\n",
    "\n",
    "# converting the clusters to a DataFrame\n",
    "customers_kmeans_pca = pd.DataFrame({'Cluster': customers_k_pca.labels_})\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(customers_kmeans_pca.iloc[: , 0].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing cluster centers\n",
    "centroids_pca = customers_k_pca.cluster_centers_\n",
    "\n",
    "\n",
    "# converting cluster centers into a DataFrame\n",
    "centroids_pca_df = pd.DataFrame(centroids_pca)\n",
    "\n",
    "\n",
    "# renaming principal components\n",
    "centroids_pca_df.columns = ['growth_mindset', \n",
    "                      'before_growth_mindset',\n",
    "                      'loyal_curious',\n",
    "                     'compulsive_theorist']\n",
    "\n",
    "\n",
    "# checking results (clusters = rows, pc = columns)\n",
    "centroids_pca_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating cluster memberships with principal components\n",
    "clst_pca_df = pd.concat([customers_kmeans_pca,\n",
    "                         all_data],\n",
    "                         axis = 1)\n",
    "\n",
    "\n",
    "# setting index\n",
    "clst_pca_df.index = demo_df.index\n",
    "\n",
    "\n",
    "# concatenating demographic information with pca-clusters\n",
    "final_pca_clust_df = pd.concat([demo_df,\n",
    "                                clst_pca_df],\n",
    "                                axis = 1)\n",
    "\n",
    "# setting index\n",
    "final_pca_clust_df.index = demo_df.index\n",
    "\n",
    "# # renaming columns\n",
    "final_pca_clust_df.columns = ['current_laptop',\n",
    "                              'next_purchase',\n",
    "                              'program',\n",
    "                              'nationality',\n",
    "                              'age_group',\n",
    "                              'willing_to_change_to_mac',\n",
    "                              'leaving_apple',\n",
    "                              'region',\n",
    "                              'gender',\n",
    "                              'cluster',\n",
    "                              'growth_mindset',\n",
    "                              'before_growth_mindset',\n",
    "                              'loyal_curious',\n",
    "                              'compulsive_theorist']\n",
    "\n",
    "\n",
    "# checking the results\n",
    "final_pca_clust_df.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ['willing_to_change_to_mac','leaving_apple','region','age_group','gender','program']\n",
    "segments = ['growth_mindset','before_growth_mindset','loyal_curious','compulsive_theorist']\n",
    "\n",
    "for elements in options:\n",
    "    for segs in segments:\n",
    "        boxplot(x = elements, y = segs, hue = 'cluster', data = final_pca_clust_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
